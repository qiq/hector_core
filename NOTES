http://docs.bor.org.ua/eBooks/Perl/Manning.Extending_and_Embedding_Perl.2002.pdf

TODO:
	- do kazdeho header souboru vlozit include <config.h>
	- preorganizovat header soubory a la featurama v *.{cc,h}
	- pridat @LOG4CXX_*@ a @LIBXML2_*@ do globalniho CXXFLAGS a LDFLAGS

rozmyslet:
- predavani dat mezi procesy + synchronizace
	- mmap + ipc
	- serializace pres protocol buffers

- resource
	- pomoci PML + prevod PML schematu do protocol buffers .proto nejakym
	  skriptem?
	- prevod do C++ struktury -- z ceho, PML schematu, nebo .proto?
	- prevod instanci PML z XML do protocol buffers

Obecne

- C++ (+ nejake bindingy?)
	- predevsim kvuli velikosti retezcu, dostupnosti knihoven
- konfigurace v XML (libxml2 pro cteni), ted se pouziva Reader, ale neni
  idealni, protoze neumi rict, na kterem radku a sloupci se stala chyba ->
  predelat na SAX2; pripadne cist po radcich (pomoci xmlReaderForIO -- vyzkouset).
- knihovny:
	- c-ares: async DNS
	- libcurl: download
	- libxml2: config soubory, html parser, uri
	- libtidy (HTML -> XHTML)
- programy:
	- djbcache: name server
	- pdns_recursor, nebo si cachovat sam(?)
- celkove: max. 8-16 GB RAM

Coding style:
- includes: C, C++ (+STL), other libs, local

Struktura

	- master, ktery zna vsechny domeny (ma v pameti)
		- muze jich byt vic, propojenych one-to-one
		- staci hashe domen (hash domain->robotid)
	- roboti, kteri znaji vsechna URL, co obsluhuji
		- maji je v pameti (teda staci jejich hashe, predpokladame, ze jsou jednoznacne)
		- + robots.txt

MASTER
	- prijima a prideluje domeny/URL ke zpracovani
	- vede si evidenci o domenach (ma v pameti)

ROBOT
	- ma seznam URL (v pameti)
	- ma obsah robots.txt ke svym domenam (taky v pameti)
	- udrzuje si fronty pozadavku na disku v souborech
	- pracuje jako s paskou, cte+modifikuje+zapisuje
	- cilova rychlost stahovani: 1000 stranek/s
	- cilovy pocet URL: 10 M na masinu (idealne vic)
		- priklady URL: vytahnout od Galambose seznam URL, zkusit zahashovat
